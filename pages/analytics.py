import streamlit as st
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from collections import Counter
import numpy as np
from scipy import stats

st.set_page_config(page_title="Statisztika", page_icon="üìä", layout="wide")
st.set_page_config(page_title="Statisztika")


# ===== CUSTOM CSS - T√ñRT√âNELMI ST√çLUS =====
st.markdown("""
<style>
    .main {
        background: linear-gradient(to bottom, #fffbf0, #fff9e6);
    }
    
    h1, h2, h3 {
        color: #2c1810 !important;
        font-family: 'Georgia', serif !important;
    }
    
    .subtitle {
        text-align: center;
        color: #5c4033;
        font-style: italic;
        font-size: 1.1rem;
        margin-bottom: 2rem;
    }
    
    .stTabs [data-baseweb="tab-list"] {
        gap: 1rem;
    }
    
    .stTabs [data-baseweb="tab"] {
        background: #fffbf0;
        border: 2px solid #d4af37;
        border-radius: 8px 8px 0 0;
        color: #2c1810;
        font-weight: bold;
    }
    
    .stTabs [aria-selected="true"] {
        background: linear-gradient(to bottom, #d4af37, #b8941f);
        color: white;
    }
</style>
""", unsafe_allow_html=True)

# ===== ADATOK BET√ñLT√âSE =====
@st.cache_data
def load_analytics_data():
    tripartit_df = pd.read_csv("data/Recept_halo__molekula_tripartit.csv", delimiter=";", encoding="utf-8")
    edges_df = pd.read_csv("data/recept_halo_edges.csv", encoding="utf-8")
    historical_df = pd.read_csv("data/HistoricalRecipe_export.csv", encoding="utf-8")
    return tripartit_df, edges_df, historical_df

tripartit_df, edges_df, historical_df = load_analytics_data()

# Sz√≥sz√°m sz√°m√≠t√°s
historical_df['word_count'] = historical_df['original_text'].fillna('').apply(lambda x: len(str(x).split()))

# ===== NODE T√çPUS MAPPING =====
# A CSV "type" oszlop√°t haszn√°ljuk (nem Intervaltype!), magyar nevekkel
type_mapping = {
    'dish': 'Recept',
    'molecule': 'Molekula',
    'ingredient': 'Alapanyag'
}

# Pr√≥b√°ljuk meg megtal√°lni a t√≠pus oszlopot (k√ºl√∂nb√∂z≈ë nevekkel)
type_column = None
for col in ['type', 'Type', 'Intervaltype', 'intervaltype']:
    if col in tripartit_df.columns:
        type_column = col
        break

if type_column:
    tripartit_df['node_type'] = tripartit_df[type_column].map(type_mapping)
    # Ha valami nem illeszkedik, "Egy√©b" kateg√≥ria
    tripartit_df['node_type'] = tripartit_df['node_type'].fillna('Egy√©b')
else:
    # Fallback: ha nincs t√≠pus oszlop, mindenki "Egy√©b"
    tripartit_df['node_type'] = 'Egy√©b'
    st.warning("‚ö†Ô∏è Nem tal√°lhat√≥ t√≠pus oszlop a CSV-ben. El√©rhet≈ë oszlopok: " + ", ".join(tripartit_df.columns.tolist()))

# ===== STATISZTIKAI ELOSZL√ÅS ELEMZ√âS =====
def analyze_distribution(data):
    """
    Elemzi az adatok eloszl√°s√°t √©s visszaadja a legjobban illeszked≈ë t√≠pust
    """
    # Alapstatisztik√°k
    mean = np.mean(data)
    median = np.median(data)
    std = np.std(data)
    skewness = stats.skew(data)
    kurtosis = stats.kurtosis(data)
    
    # Normalit√°s teszt (Shapiro-Wilk)
    # Ha p > 0.05, akkor norm√°lis eloszl√°s
    if len(data) < 5000:  # Shapiro-Wilk max 5000 mint√°ra m≈±k√∂dik j√≥l
        shapiro_stat, shapiro_p = stats.shapiro(data)
    else:
        # Nagy mint√°kra Anderson-Darling teszt
        anderson_result = stats.anderson(data, dist='norm')
        shapiro_p = 0.05 if anderson_result.statistic > anderson_result.critical_values[2] else 0.1
    
    # K√ºl√∂nb√∂z≈ë eloszl√°sokhoz illeszt√©s
    distributions = {
        'norm': 'Norm√°lis',
        'lognorm': 'Lognorm√°lis',
        'expon': 'Exponenci√°lis',
        'gamma': 'Gamma',
        'weibull_min': 'Weibull'
    }
    
    best_fit = None
    best_ks_stat = float('inf')
    best_dist_name = None
    
    for dist_name, dist_label in distributions.items():
        try:
            # Param√©ter illeszt√©s
            params = getattr(stats, dist_name).fit(data)
            # Kolmogorov-Smirnov teszt
            ks_stat, ks_p = stats.kstest(data, dist_name, args=params)
            
            if ks_stat < best_ks_stat:
                best_ks_stat = ks_stat
                best_fit = dist_label
                best_dist_name = dist_name
        except:
            continue
    
    # Eloszl√°s t√≠pus meghat√°roz√°sa a tulajdons√°gok alapj√°n
    if shapiro_p > 0.05:
        dist_type = "‚úÖ **Norm√°lis eloszl√°s**"
        explanation = "Az adatok norm√°lis eloszl√°st k√∂vetnek (Shapiro-Wilk p > 0.05)"
    elif skewness > 1:
        dist_type = "üìà **Jobbra ferde eloszl√°s**"
        explanation = f"Er≈ësen jobbra ferde (ferdes√©g: {skewness:.2f}). Van n√©h√°ny extr√©m nagy √©rt√©k."
    elif skewness > 0.5:
        dist_type = "üìä **M√©rs√©kelten jobbra ferde**"
        explanation = f"Jobbra ferde (ferdes√©g: {skewness:.2f}). Az √°tlag > medi√°n."
    elif skewness < -1:
        dist_type = "üìâ **Balra ferde eloszl√°s**"
        explanation = f"Er≈ësen balra ferde (ferdes√©g: {skewness:.2f})"
    elif abs(skewness) < 0.5:
        dist_type = "‚öñÔ∏è **Szimmetrikus eloszl√°s**"
        explanation = f"K√∂zel szimmetrikus (ferdes√©g: {skewness:.2f})"
    else:
        dist_type = "üìä **Aszimmetrikus eloszl√°s**"
        explanation = f"Balra ferde (ferdes√©g: {skewness:.2f})"
    
    # Cs√∫csoss√°g √©rtelmez√©se
    if kurtosis > 3:
        kurtosis_type = "üî∫ Leptokurtikus (cs√∫csos)"
    elif kurtosis < -3:
        kurtosis_type = "üîª Platykurtikus (lapos)"
    else:
        kurtosis_type = "‚ö´ Mezokurtikus (norm√°lis cs√∫csoss√°g)"
    
    return {
        'type': dist_type,
        'explanation': explanation,
        'best_fit': best_fit,
        'skewness': skewness,
        'kurtosis': kurtosis,
        'kurtosis_type': kurtosis_type,
        'shapiro_p': shapiro_p if 'shapiro_p' in locals() else None,
        'mean': mean,
        'median': median,
        'std': std
    }

# ===== HEADER =====
st.title("üìä Korpusz Analitika Dashboard")
st.markdown('<div class="subtitle">H√°l√≥zati statisztik√°k, recept hossz√∫s√°g eloszl√°s √©s AI gener√°l√°si strat√©gi√°k</div>', unsafe_allow_html=True)

# Dekorat√≠v elv√°laszt√≥
st.markdown("""
<div style="width: 150px; height: 3px; background: linear-gradient(to right, #d4af37, #f0d98d, #d4af37); 
            margin: 0 auto 2rem auto; border-radius: 2px;"></div>
""", unsafe_allow_html=True)

# ===== TAB-ok =====
tab1, tab2, tab3, tab4 = st.tabs([
    "üï∏Ô∏è H√°l√≥zati Elemz√©s",
    "üìè Recept Hossz√∫s√°g",
    "ü§ñ AI Strat√©gi√°k",
    "üìö T√∂rt√©neti Korpusz"
])

# ===== TAB 1: H√ÅL√ìZATI ELEMZ√âS =====
with tab1:
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("### üî¢ Alapstatisztik√°k")
        
        metrics_col1, metrics_col2, metrics_col3 = st.columns(3)
        metrics_col1.metric("Node-ok sz√°ma", len(tripartit_df))
        metrics_col2.metric("Kapcsolatok sz√°ma", len(edges_df))
        metrics_col3.metric("√Åtlagos degree", round(tripartit_df['Degree'].mean(), 2))
        
        # Degree eloszl√°s histogram
        st.markdown("### üìä Degree Eloszl√°s")
        fig_degree = go.Figure()
        
        # Histogram
        fig_degree.add_trace(go.Histogram(
            x=tripartit_df['Degree'],
            nbinsx=30,
            marker_color='#8b5a2b',
            name='Degree',
            opacity=0.7,
            histnorm='probability density'
        ))
        
        # Illesztett eloszl√°s g√∂rbe
        degree_data = tripartit_df['Degree'].values
        x_range = np.linspace(degree_data.min(), degree_data.max(), 100)
        
        # Norm√°lis eloszl√°s g√∂rbe
        mu, sigma = degree_data.mean(), degree_data.std()
        normal_curve = stats.norm.pdf(x_range, mu, sigma)
        fig_degree.add_trace(go.Scatter(
            x=x_range,
            y=normal_curve,
            mode='lines',
            name='Norm√°lis illeszt√©s',
            line=dict(color='red', width=2, dash='dash')
        ))
        
        # Lognorm√°lis illeszt√©s (ha ez a legjobb)
        try:
            shape, loc, scale = stats.lognorm.fit(degree_data, floc=0)
            lognorm_curve = stats.lognorm.pdf(x_range, shape, loc, scale)
            fig_degree.add_trace(go.Scatter(
                x=x_range,
                y=lognorm_curve,
                mode='lines',
                name='Lognorm√°lis illeszt√©s',
                line=dict(color='green', width=3)
            ))
        except:
            pass
        
        fig_degree.update_layout(
            xaxis_title="Degree",
            yaxis_title="S≈±r≈±s√©g",
            paper_bgcolor='#fcf5e5',
            plot_bgcolor='#fcf5e5',
            height=400,
            showlegend=True
        )
        st.plotly_chart(fig_degree, use_container_width=True)
        
        # Eloszl√°s anal√≠zis a Degree-re
        st.markdown("### üìà Degree Eloszl√°s Elemz√©se")
        degree_analysis = analyze_distribution(tripartit_df['Degree'].values)
        
        st.info(f"""
        **{degree_analysis['type']}**
        
        {degree_analysis['explanation']}
        
        - **Legjobb illeszked√©s:** {degree_analysis['best_fit']}
        - **Ferdes√©g (skewness):** {degree_analysis['skewness']:.3f}
        - **Cs√∫csoss√°g (kurtosis):** {degree_analysis['kurtosis']:.3f} ‚Äî {degree_analysis['kurtosis_type']}
        """)

    
    with col2:
        st.markdown("### üé® Node T√≠pusok")
        
        type_counts = tripartit_df['node_type'].value_counts()
        
        # R√©szletes statisztika
        st.markdown("#### üìä T√≠pus Eloszl√°s")
        for node_type, count in type_counts.items():
            percent = (count / len(tripartit_df)) * 100
            emoji = {'Alapanyag': 'ü•ò', 'Molekula': '‚öóÔ∏è', 'Recept': 'üìñ', 'Egy√©b': '‚ö™'}.get(node_type, '‚ö™')
            st.markdown(f"{emoji} **{node_type}:** {count} db ({percent:.1f}%)")
        
        # Pie chart
        fig_types = go.Figure(data=[go.Pie(
            labels=type_counts.index,
            values=type_counts.values,
            marker=dict(colors=['#8b5a2b', '#4a7c59', '#b85450', '#cccccc']),
            hole=0.4
        )])
        fig_types.update_layout(
            paper_bgcolor='#fcf5e5',
            height=350
        )
        st.plotly_chart(fig_types, use_container_width=True)
        
        # Top 10 legnagyobb degree
        st.markdown("### üèÜ Top 10 Node (degree szerint)")
        top_nodes = tripartit_df.nlargest(10, 'Degree')[['Label', 'Degree', 'node_type']]
        
        for idx, row in top_nodes.iterrows():
            emoji = {'Alapanyag': 'ü•ò', 'Molekula': '‚öóÔ∏è', 'Recept': 'üìñ', 'Egy√©b': '‚ö™'}.get(row['node_type'], '‚ö™')
            st.markdown(f"{emoji} **{row['Label']}** - Degree: {row['Degree']}")

# ===== TAB 2: RECEPT HOSSZ√öS√ÅG =====
with tab2:
    st.markdown("### üìè Recept Hossz√∫s√°g Statisztik√°k")
    
    # Le√≠r√≥ stat
    stats_col1, stats_col2, stats_col3, stats_col4, stats_col5 = st.columns(5)
    stats_col1.metric("Receptek sz√°ma", len(historical_df))
    stats_col2.metric("√Åtlag sz√≥", round(historical_df['word_count'].mean(), 1))
    stats_col3.metric("Medi√°n sz√≥", int(historical_df['word_count'].median()))
    stats_col4.metric("Min sz√≥", int(historical_df['word_count'].min()))
    stats_col5.metric("Max sz√≥", int(historical_df['word_count'].max()))
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("### üìä Sz√≥sz√°m Eloszl√°s Histogram")
        
        fig_hist = go.Figure()
        
        # Histogram
        fig_hist.add_trace(go.Histogram(
            x=historical_df['word_count'],
            nbinsx=40,
            marker_color='#8b5a2b',
            name='Sz√≥sz√°m',
            opacity=0.7,
            histnorm='probability density'
        ))
        
        # Illesztett eloszl√°s g√∂rb√©k
        word_data = historical_df['word_count'].values
        x_range = np.linspace(word_data.min(), word_data.max(), 200)
        
        # Norm√°lis eloszl√°s g√∂rbe (√∂sszehasonl√≠t√°shoz)
        mu, sigma = word_data.mean(), word_data.std()
        normal_curve = stats.norm.pdf(x_range, mu, sigma)
        fig_hist.add_trace(go.Scatter(
            x=x_range,
            y=normal_curve,
            mode='lines',
            name='Norm√°lis (elm√©leti)',
            line=dict(color='red', width=2, dash='dash')
        ))
        
        # Lognorm√°lis illeszt√©s (val√≥s eloszl√°s)
        try:
            shape, loc, scale = stats.lognorm.fit(word_data, floc=0)
            lognorm_curve = stats.lognorm.pdf(x_range, shape, loc, scale)
            fig_hist.add_trace(go.Scatter(
                x=x_range,
                y=lognorm_curve,
                mode='lines',
                name='Lognorm√°lis (illesztett)',
                line=dict(color='green', width=3)
            ))
        except:
            pass
        
        # √Åtlag √©s medi√°n vonalak
        fig_hist.add_vline(
            x=historical_df['word_count'].mean(),
            line_dash="dash",
            line_color="darkred",
            annotation_text=f"√Åtlag: {historical_df['word_count'].mean():.1f}",
            annotation_position="top"
        )
        fig_hist.add_vline(
            x=historical_df['word_count'].median(),
            line_dash="dash",
            line_color="darkblue",
            annotation_text=f"Medi√°n: {historical_df['word_count'].median():.0f}",
            annotation_position="top"
        )
        
        fig_hist.update_layout(
            xaxis_title="Sz√≥sz√°m",
            yaxis_title="S≈±r≈±s√©g",
            paper_bgcolor='#fcf5e5',
            plot_bgcolor='#fcf5e5',
            height=450,
            showlegend=True,
            legend=dict(
                yanchor="top",
                y=0.99,
                xanchor="right",
                x=0.99
            )
        )
        st.plotly_chart(fig_hist, use_container_width=True)
        
        # Eloszl√°s anal√≠zis a sz√≥sz√°mra
        st.markdown("### üìà Eloszl√°s Elemz√©se")
        word_analysis = analyze_distribution(historical_df['word_count'].values)
        
        st.success(f"""
        **{word_analysis['type']}**
        
        {word_analysis['explanation']}
        
        **Statisztikai jellemz≈ëk:**
        - **Legjobb illeszked√©s:** {word_analysis['best_fit']}
        - **Ferdes√©g (skewness):** {word_analysis['skewness']:.3f}
        - **Cs√∫csoss√°g (kurtosis):** {word_analysis['kurtosis']:.3f} ‚Äî {word_analysis['kurtosis_type']}
        - **Sz√≥r√°s:** {word_analysis['std']:.1f} sz√≥
        """)
        
        if word_analysis['skewness'] > 0.5:
            st.warning("""
            ‚ö†Ô∏è **Jobbra ferde eloszl√°s magyar√°zat:**
            
            **"Jobbra ferde" = a hossz√∫ farok jobbra (nagy √©rt√©kek fel√©) ny√∫lik**
            
            Ez azt jelenti:
            - üìä A legt√∂bb recept **r√∂vid** (30-70 sz√≥)
            - üìà Van n√©h√°ny **extr√©m hossz√∫** recept (200+ sz√≥) ‚Üí ezek "h√∫zz√°k jobbra" a farok v√©g√©t
            - ‚öñÔ∏è **√Åtlag > Medi√°n** (az outlier-ek felh√∫zz√°k az √°tlagot)
            - üéØ A **medi√°n megb√≠zhat√≥bb** mint az √°tlag
            
            **P√©lda:** Ha 90% r√∂vid (50 sz√≥), de van 10 db 300+ szavas recept ‚Üí 
            az √°tlag magasabb lesz, mint a tipikus recept hossza.
            """)
        
        # Vizu√°lis magyar√°zat diagram
        st.markdown("#### üé® Eloszl√°s Vizualiz√°ci√≥")
        col_viz1, col_viz2 = st.columns(2)
        
        with col_viz1:
            st.markdown("""
            **üî¥ Piros vonal (Norm√°lis):** Szimmetrikus harangg√∂rbe
            - √Åtlag = Medi√°n = M√≥dusz
            - Nincs hossz√∫ farok
            """)
        
        with col_viz2:
            st.markdown("""
            **üü¢ Z√∂ld vonal (Lognorm√°lis):** Jobbra ferde
            - √Åtlag > Medi√°n
            - Hossz√∫ jobb oldali farok
            - Ez illeszkedik az adatainkra!
            """)

    
    with col2:
        st.markdown("### ü•ß Hossz√∫s√°g Kateg√≥ri√°k")
        
        # Kategoriz√°l√°s
        def categorize_length(word_count):
            if word_count <= 30:
                return 'Nagyon r√∂vid (‚â§30)'
            elif word_count <= 60:
                return 'R√∂vid (31-60)'
            elif word_count <= 100:
                return 'K√∂zepes (61-100)'
            elif word_count <= 200:
                return 'Hossz√∫ (101-200)'
            else:
                return 'Nagyon hossz√∫ (>200)'
        
        historical_df['length_category'] = historical_df['word_count'].apply(categorize_length)
        category_counts = historical_df['length_category'].value_counts()
        
        colors = {
            'Nagyon r√∂vid (‚â§30)': '#8b5a2b',
            'R√∂vid (31-60)': '#a67c52',
            'K√∂zepes (61-100)': '#c9a877',
            'Hossz√∫ (101-200)': '#dcc5a0',
            'Nagyon hossz√∫ (>200)': '#f0e5d3'
        }
        
        fig_pie = go.Figure(data=[go.Pie(
            labels=category_counts.index,
            values=category_counts.values,
            marker=dict(colors=[colors[cat] for cat in category_counts.index]),
            textinfo='label+percent',
            textposition='outside'
        )])
        fig_pie.update_layout(
            paper_bgcolor='#fcf5e5',
            height=400
        )
        st.plotly_chart(fig_pie, use_container_width=True)
        
        # R√©szletes lebont√°s
        st.markdown("#### üìã R√©szletes Eloszl√°s")
        for cat, count in category_counts.items():
            percent = (count / len(historical_df)) * 100
            st.markdown(f"**{cat}:** {count} db ({percent:.1f}%)")

# ===== TAB 3: AI STRAT√âGI√ÅK =====
with tab3:
    st.markdown("### ü§ñ AI Gener√°l√°si Strat√©gi√°k (GPT-5.1 & GPT-5-mini Best Practices)")
    
    strategies = [
        {
            "mode": "minimal",
            "trigger": "0 p√©lda VAGY √°tlag degree < 3",
            "word_target": "max 40 sz√≥",
            "style": "Eml√©keztet≈ë st√≠lus, minim√°lis kontextus",
            "use_case": "Gyenge h√°l√≥zati alap, nincs t√∂rt√©neti p√©lda",
            "prompt_key": "Ultra-concise, grounding warnings",
            "color": "#b85450"
        },
        {
            "mode": "concise",
            "trigger": "1-2 p√©lda",
            "word_target": "40-70 sz√≥",
            "style": "Lakonikus, tapasztalt szak√°cs st√≠lus",
            "use_case": "Kev√©s p√©lda, k√∂zepes kapcsol√≥d√°s",
            "prompt_key": "Terse instructions, assumed knowledge",
            "color": "#8b5a2b"
        },
        {
            "mode": "standard",
            "trigger": "3-5 p√©lda",
            "word_target": "70-110 sz√≥",
            "style": "Klasszikus 17. sz√°zadi recept forma",
            "use_case": "K√∂zepes p√©ldat√°r, er≈ës h√°l√≥zat",
            "prompt_key": "Complete but compact, contextual",
            "color": "#4a7c59"
        },
        {
            "mode": "detailed",
            "trigger": "6+ p√©lda",
            "word_target": "110-160 sz√≥",
            "style": "R√©szletes technol√≥giai le√≠r√°s kontextussal",
            "use_case": "Gazdag forr√°s, kiv√°l√≥ kapcsol√≥d√°s",
            "prompt_key": "Step-by-step, timing, cultural context",
            "color": "#d4af37"
        }
    ]
    
    for strategy in strategies:
        with st.expander(f"**{strategy['mode'].upper()}** - {strategy['word_target']}", expanded=True):
            col1, col2 = st.columns([3, 1])
            
            with col1:
                st.markdown(f"**Trigger felt√©tel:** {strategy['trigger']}")
                st.markdown(f"**St√≠lus:** {strategy['style']}")
                st.markdown(f"**Haszn√°lat:** {strategy['use_case']}")
                st.markdown(f"**Prompt kulcselem:** `{strategy['prompt_key']}`")
            
            with col2:
                st.markdown(
                    f"<div style='background-color: {strategy['color']}; "
                    f"color: white; padding: 20px; border-radius: 10px; text-align: center;'>"
                    f"<h3 style='margin: 0;'>{strategy['word_target']}</h3>"
                    f"</div>",
                    unsafe_allow_html=True
                )
    
    st.markdown("---")
    st.markdown("### üìñ GPT-5-mini Prompt Engineering Principles")
    
    principles = [
        ("üéØ Grounding & Accuracy", "SOHA ne tal√°lj ki adatokat - csak h√°l√≥zati kapcsolatok alapj√°n"),
        ("üìè Verbosity Control", "Adapt√≠v hossz√∫s√°g: p√©ldasz√°m + h√°l√≥zati er≈ëss√©g alapj√°n"),
        ("üîç Network-Informed", "Degree-s√∫lyozott d√∂nt√©sek (magas degree = er≈ës p√°ros√≠t√°s)"),
        ("‚ö†Ô∏è Uncertainty Handling", "Explicit confidence score (low/medium/high)"),
        ("‚úÖ Self-Check", "Gener√°l√°s el≈ëtti valid√°ci√≥: kapcsolatok, forr√°sok, hossz"),
        ("üìä Structured Output", "JSON schema strict enforcement")
    ]
    
    for title, desc in principles:
        st.markdown(f"**{title}:** {desc}")

# ===== TAB 4: T√ñRT√âNETI KORPUSZ =====
with tab4:
    st.markdown("### üìö T√∂rt√©neti Receptek B√∂ng√©sz√©se")
    
    # Keres√©s
    search = st.text_input("üîç Keres√©s a receptekben", placeholder="Pl. hal, bors, leves...")
    
    # Sz≈±r√©s hossz szerint
    col1, col2 = st.columns([1, 3])
    with col1:
        length_filter = st.selectbox(
            "Hossz√∫s√°g sz≈±r≈ë",
            ["√ñsszes", "Nagyon r√∂vid (‚â§30)", "R√∂vid (31-60)", "K√∂zepes (61-100)", "Hossz√∫ (101-200)", "Nagyon hossz√∫ (>200)"]
        )
    
    # Sz≈±rt adatok
    filtered_df = historical_df.copy()
    
    if search:
        filtered_df = filtered_df[
            filtered_df['title'].fillna('').str.contains(search, case=False) |
            filtered_df['original_text'].fillna('').str.contains(search, case=False)
        ]
    
    if length_filter != "√ñsszes":
        filtered_df = filtered_df[filtered_df['length_category'] == length_filter]
    
    st.markdown(f"**Tal√°latok:** {len(filtered_df)} recept")
    
    # Megjelen√≠t√©s
    for idx, row in filtered_df.head(20).iterrows():
        with st.expander(f"üìñ {row['title']} ({row['word_count']} sz√≥) - {row.get('source', 'Ismeretlen forr√°s')}"):
            st.markdown(f"**Kateg√≥ria:** {row['length_category']}")
            st.markdown("---")
            st.markdown(row['original_text'][:500] + ("..." if len(str(row['original_text'])) > 500 else ""))

st.markdown("---")
st.markdown(
    "<div style='text-align: center; color: #8b5a2b;'>"
    f"Analytics Dashboard ¬© 2025 | Korpusz: {len(historical_df)} recept, {historical_df['word_count'].mean():.1f} sz√≥ √°tlag"
    "</div>",
    unsafe_allow_html=True

)


