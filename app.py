import streamlit as st
import streamlit.components.v1 as components
import networkx as nx
import plotly.graph_objects as go
import json
import pandas as pd
import os
from dotenv import load_dotenv
from openai import OpenAI
import html as _html
import textwrap
import random
import unicodedata
import re
import base64
import difflib

st.set_page_config(
    page_title="K√∂zrendek √çzh√°l√≥ja",
    page_icon="üìú",
    layout="wide",
    initial_sidebar_state="collapsed"
)

st.markdown(""" 
<style>
div[data-baseweb="select"] > div {
    background-color: #7a0f0f !important;
    color: #f5f5f5 !important;
    border-radius: 10px;
    border: 1px solid #cfa34a;
}
div[data-baseweb="popover"] { background-color: #2a0c0c !important; border-radius: 12px; border: 1px solid #cfa34a; }
div[data-baseweb="menu"] { background-color: #2a0c0c !important; }
div[data-baseweb="option"] { color: #f0e6d2 !important; background-color: transparent !important; }
div[data-baseweb="option"]:hover { background-color: #7a0f0f !important; color: #ffffff !important; }
</style>
""", unsafe_allow_html=True)

st.markdown(""" 
<style>
    @import url('https://fonts.googleapis.com/css2?family=Cinzel:wght@400;700;900&family=Crimson+Text:ital,wght@0,400;0,600;0,700;1,400&display=swap');
    .main { background: linear-gradient(135deg, #1a1a1a 0%, #2d2d2d 50%, #1a1a1a 100%) !important; background-image: url("https://www.transparenttextures.com/patterns/dark-leather.png") !important; padding: 0 !important; }
    .block-container { padding: 2rem 3rem !important; max-width: 1400px !important; background: rgba(0, 0, 0, 0.3); }
    h1, h2, h3 { font-family: 'Cinzel', serif !important; color: white !important; font-weight: 700 !important; }
    h1 { font-size: 2.5rem !important; text-align: center !important; margin-bottom: 1rem !important; }
    .block-container p, .block-container div, .block-container span, .block-container li { font-family: 'Crimson Text', serif !important; color: white !important; font-size: 1.05rem; }
    .stButton > button { background: linear-gradient(135deg, #800000 0%, #5c1a1a 100%); color: white !important; border: none; border-radius: 8px; font-family: 'Cinzel', serif !important; font-size: 1rem !important; font-weight: 600 !important; padding: 0.6rem 1rem; box-shadow: 0 2px 4px rgba(0, 0, 0, 0.5); transition: all 0.3s ease; width: 100%; text-align: left; }
    .stButton > button:hover { transform: translateY(-2px); box-shadow: 0 4px 8px rgba(204, 170, 119, 0.3); background: linear-gradient(135deg, #a52a2a 0%, #722828 100%); }
    div[data-testid="stMetric"] { background: linear-gradient(135deg, #2d2d2d 0%, #1a1a1a 100%); padding: 1.5rem; border-radius: 12px; border: 2px solid #ccaa77; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.5); }
    [data-testid="stMetricValue"] { font-family: 'Cinzel', serif !important; color: white !important; font-size: 2.5rem !important; font-weight: 700 !important; }
    [data-testid="stMetricLabel"] { font-family: 'Crimson Text', serif !important; color: white !important; font-size: 1rem !important; text-transform: uppercase; letter-spacing: 1px; }
    .stTextInput input, .stTextInput div[role="textbox"] input { background-color: #840A13 !important; color: #f5efe6 !important; }
    .stTextInput input::placeholder { color: #f5efe6 !important; opacity: 0.9 !important; font-style: italic; }
    div[data-testid="stSelectbox"] div[role="listbox"], div[data-baseweb="menu"] [role="listbox"], div[role="listbox"], div[role="presentation"] > div[role="listbox"], div[role="menu"], div[data-testid="stSelectbox"] .baseweb-popover-content, .rc-virtual-list, .baseweb-popover-content { background-color: #4a0d0d !important; color: #f5efe6 !important; border-radius: 10px !important; box-shadow: 0 10px 30px rgba(0,0,0,0.6) !important; max-height: 360px !important; overflow-y: auto !important; min-width: 260px !important; width: auto !important; padding: 0.2rem !important; z-index: 100001 !important; border: 1px solid rgba(255,36,0,0.12) !important; }
    div[data-testid="stSelectbox"] div[role="listbox"] *, div[data-testid="stSelectbox"] .baseweb-popover-content * { background-color: transparent !important; color: inherit !important; }
    div[data-testid="stSelectbox"] div[role="listbox"] [role="option"], .baseweb-popover-content [role="option"], div[role="option"] { background-color: transparent !important; color: #f5efe6 !important; padding: 0.6rem 0.9rem !important; }
    div[data-testid="stSelectbox"] div[role="listbox"]::-webkit-scrollbar, .baseweb-popover-content::-webkit-scrollbar { width: 10px !important; height: 10px !important; }
    div[data-testid="stSelectbox"] div[role="listbox"]::-webkit-scrollbar-thumb, .baseweb-popover-content::-webkit-scrollbar-thumb { background: rgba(0,0,0,0.35) !important; border-radius: 8px !important; border: 2px solid rgba(255,255,255,0.02) !important; }
    div[data-testid="stSelectbox"] div[role="listbox"], div[data-baseweb="menu"] [role="listbox"], div[role="listbox"], div[role="presentation"] > div[role="listbox"], div[role="menu"] { background-color: #840A13 !important; color: #f5efe6 !important; border-radius: 10px !important; box-shadow: 0 10px 30px rgba(0,0,0,0.6) !important; max-height: 360px !important; overflow-y: auto !important; min-width: 260px !important; width: auto !important; padding: 0.2rem !important; z-index: 100001 !important; }
    div[role="option"] { background-color: transparent !important; color: #f5efe6 !important; padding: 0.6rem 0.9rem !important; font-family: 'Crimson Text', serif !important; font-size: 1rem !important; cursor: pointer !important; border-radius: 6px !important; margin: 0.12rem 0 !important; }
    div[role="option"]:hover, div[role="option"][data-highlighted="true"], div[role="option"][aria-selected="true"] { background-color: #FF2400 !important; color: #ffffff !important; font-weight: 600 !important; }
    div[data-testid="stSelectbox"] div[data-baseweb="select"] > div { background-color: #840A13 !important; border: 2px solid #FF2400 !important; border-radius: 8px !important; color: #f5efe6 !important; z-index: 99999 !important; }
    div[data-testid="stSelectbox"] input, div[data-testid="stSelectbox"] [role="combobox"], div[data-testid="stSelectbox"] [role="button"] { color: #f5efe6 !important; background-color: #840A13 !important; }
    div[data-testid="stSelectbox"] div[data-baseweb="select"] > div > span { color: #f5efe6 !important; }
    @media (max-width: 800px) {
        div[data-testid="stSelectbox"] div[role="listbox"], div[role="listbox"] { left: 1rem !important; right: 1rem !important; width: auto !important; min-width: unset !important; }
    }
    [data-testid="stSidebar"] > div:first-child { background-color: #5c1a1a !important; font-family: 'Cinzel', serif !important; color: #ffffff !important; }
    [data-testid="stSidebar"] button, [data-testid="stSidebar"] .st-expander, [data-testid="stSidebar"] span, [data-testid="stSidebar"] div[data-testid$="-label"] { font-family: 'Cinzel', serif !important; color: #ffffff !important; }
    [data-testid="stSidebar"] span[data-testid="stIconMaterial"], .span[data-testid="stIconMaterial"] { display: none !important; }
    [data-testid="stKeyboardShortcutButton"], button[aria-label="Show keyboard shortcuts"], button[aria-label="Show keyboard navigation"], [data-testid^="stTooltip"] { display: none !important; }
    .carousell-card { background: linear-gradient(135deg, #1a1a1a, #1f1f1f); border-radius: 18px; border: 1px solid #d4af37; box-shadow: 0 15px 30px rgba(0, 0, 0, 0.4); padding: 24px; color: #f9f3e8; font-family: 'Playfair Display', serif; }
    .card-title { font-size: 26px; margin-bottom: 12px; text-transform: uppercase; letter-spacing: 0.08em; }
    .card-value { font-size: 42px; margin: 0; font-weight: 600; }
    .card-desc { font-size: 16px; line-height: 1.6; margin-top: 12px; color: #e7dac5; }
</style>
""", unsafe_allow_html=True)

load_dotenv()
api_key = None
try:
    api_key = st.secrets.get("OPENAI_API_KEY")
except Exception:
    api_key = None
if not api_key:
    api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    st.error("‚ùå Hi√°nyzik az OPENAI_API_KEY! Add it to `.streamlit/secrets.toml` or set the OPENAI_API_KEY environment variable.")
    st.stop()
client = OpenAI(api_key=api_key)
random.seed(42)

def strip_icon_ligatures(s: str) -> str:
    if not isinstance(s, str):
        return s
    s = _html.unescape(s)
    s = re.sub(r"<[^>]+>", "", s)
    s = unicodedata.normalize('NFKC', s)
    filtered_chars = []
    for ch in s:
        cat = unicodedata.category(ch)
        o = ord(ch)
        if cat.startswith('C'):
            continue
        if 0xE000 <= o <= 0xF8FF:
            continue
        if 0xF0000 <= o <= 0xFFFFD:
            continue
        filtered_chars.append(ch)
    s = ''.join(filtered_chars)
    s = re.sub(r'[_\-\s]+', ' ', s).strip()
    icon_keywords = {'keyboard','keyb','arrow','check','radio','menu','close','settings','search','favorite','share','more','material','icon','icons','vert','horiz'}
    def token_clean(t: str) -> str:
        t_norm = re.sub(r'[^a-z0-9]+', '', t.lower())
        return t_norm
    tokens = [t for t in s.split() if not any(kw in token_clean(t) for kw in icon_keywords)]
    s = ' '.join(tokens)
    s = re.sub(r'\s{2,}', ' ', s).strip()
    return s

@st.cache_data
def load_data():
    script_dir = os.path.dirname(__file__)
    def _resolve(rel_path):
        candidates = []
        bases = [script_dir, os.getcwd(), os.path.abspath(os.path.join(script_dir, '..'))]
        for b in bases:
            candidates.append(os.path.normpath(os.path.join(b, rel_path)))
        candidates.append(os.path.normpath(rel_path))
        for p in candidates:
            if os.path.exists(p):
                return p
        return candidates

    tripartit_path = _resolve(os.path.join('data', 'Recept_halo__molekula_tripartit.csv'))
    edges_path = _resolve(os.path.join('data', 'recept_halo_edges.csv'))
    historical_path = _resolve(os.path.join('data', 'HistoricalRecipe_export.csv'))

    def _ensure_found(res, logical_name):
        if isinstance(res, list):
            st.error(f"‚ùå Hi√°nyzik a f√°jl: {logical_name}. Pr√≥b√°lt el√©r√©si utak:")
            for p in res:
                st.write(f"- {p}")
            st.stop()
        return res

    tripartit_path = _ensure_found(tripartit_path, 'data/Recept_halo__molekula_tripartit.csv')
    edges_path = _ensure_found(edges_path, 'data/recept_halo_edges.csv')
    historical_path = _ensure_found(historical_path, 'data/HistoricalRecipe_export.csv')

    def safe_read_csv(path, name, default_sep=';'):
        try:
            return pd.read_csv(path, delimiter=default_sep, encoding='utf-8', on_bad_lines='skip')
        except Exception:
            try:
                return pd.read_csv(path, sep=None, engine='python', encoding='utf-8', on_bad_lines='skip')
            except Exception:
                try:
                    return pd.read_csv(path, delimiter=default_sep, encoding='latin1', on_bad_lines='skip')
                except Exception:
                    try:
                        return pd.read_csv(path, sep=None, engine='python', encoding='latin1', on_bad_lines='skip')
                    except Exception:
                        try:
                            with open(path, 'r', encoding='utf-8', errors='replace') as fh:
                                preview = fh.read(5000)
                        except Exception:
                            preview = f"(Could not read file contents for preview: {path})"
                        st.error(f"‚ùå Hiba a CSV beolvas√°sakor: {name}")
                        st.markdown("**Pr√≥b√°lt beolvas√°si m√≥dszerek:** UTF-8 with ';', infer sep (python engine), Latin-1 variants.")
                        st.markdown("**F√°jl el≈ën√©zet (els≈ë 5000 karakter):**")
                        st.code(preview)
                        st.stop()

    tripartit_df = safe_read_csv(tripartit_path, 'data/Recept_halo__molekula_tripartit.csv', default_sep=';')
    edges_df = safe_read_csv(edges_path, 'data/recept_halo_edges.csv', default_sep=',')
    historical_df = safe_read_csv(historical_path, 'data/HistoricalRecipe_export.csv', default_sep=',')

    for col in ['title', 'original_text', 'ingredients']:
        if col in historical_df.columns:
            historical_df[col] = historical_df[col].apply(lambda x: strip_icon_ligatures(x) if isinstance(x, str) else x)

    perfect_ings = []
    try:
        perfect_candidate = _resolve(os.path.join('data', 'recept_alapanyagok_T√ñK√âLETES.json'))
        if not isinstance(perfect_candidate, list) and os.path.exists(perfect_candidate):
            with open(perfect_candidate, encoding='utf-8') as f:
                raw = json.load(f)
                ingredients = set()
                if isinstance(raw, dict):
                    for v in raw.values():
                        if isinstance(v, list):
                            for item in v:
                                if isinstance(item, str):
                                    ingredients.add(item)
                        elif isinstance(v, str):
                            ingredients.add(v)
                elif isinstance(raw, list):
                    for entry in raw:
                        if isinstance(entry, str):
                            ingredients.add(entry)
                        elif isinstance(entry, dict):
                            for v in entry.values():
                                if isinstance(v, list):
                                    for item in v:
                                        if isinstance(item, str):
                                            ingredients.add(item)
                                elif isinstance(v, str):
                                    ingredients.add(v)
                perfect_ings = sorted(ingredients)
    except Exception:
        perfect_ings = []

    return tripartit_df, edges_df, historical_df, perfect_ings

tripartit_df, edges_df, historical_df, perfect_ings = load_data()

SYNONYM_MAP = {
    "r√≥zsabors": ["r√≥zsabors", "pink pepper", "schinus", "schinus molle", "r√≥zs√°s bors"],
    "avok√°d√≥": ["avok√°d√≥", "avokado", "avok√°d√≥s", "avocado"],
    "avok√°d√≥s": ["avok√°d√≥", "avokado", "avok√°d√≥s", "avocado"],
    "kaja": ["kaja", "√©tel", "fog√°s", "meal", "dish", "food"],
    "mandula": ["mandula", "almond"]
}
GENERIC_TOKENS = {"kaja", "√©tel", "fog√°s", "recept", "food", "dish", "meal"}

TOKEN_ROLE = {
  "ingredient": [],
  "flavour_descriptor": [],
  "preparation_style": [],
  "generic_food": [],
  "metaphorical": [],
}

ANACHRONISTIC_INGREDIENTS = {
  "avok√°d√≥", "paradicsom", "burgonya", "csili", "van√≠lia", "kaka√≥", "paprika", "anan√°sz"
}

HISTORICAL_ANALOGY_MAP = {
    "avok√°d√≥": ["mandula", "olaj-sp√©k", "t√∂rt h√ºvelyes"],
    "avok√°d√≥s": ["mandula", "olaj-sp√©k", "t√∂rt h√ºvelyes"],
    "r√≥zsabors": ["tiszta borssal", "r√≥zsaszirom inf√∫zi√≥ (illatos√≠t√≥)", "borsos szilva"],
    "pink pepper": ["tiszta borssal", "r√≥zsaszirom inf√∫zi√≥ (illatos√≠t√≥)"]
}

HISTORICAL_DISH_STRUCTURE_MAP = {
    "pite": {
        "interpreted_as": "t√∂lt√∂tt vagy r√©tegezett t√©szt√°s √©tel",
        "historical_equivalents": [
            "t√∫r√≥s t√©szta",
            "alm√°s t√©szta",
            "m√°kos t√©szta",
            "k√°poszt√°s t√©szta"
        ],
        "confidence": 0.75,
        "source": "MEK t√∂rt√©neti receptgy≈±jtem√©ny"
    }
}

def detect_label_col(df):
    candidates = [c for c in df.columns if c.lower() in ('label','name','title','node','node_name')]
    if candidates:
        return candidates[0]
    for c in df.columns:
        if 'label' in c.lower() or 'name' in c.lower():
            return c
    return df.columns[0] if len(df.columns) else None

label_col = detect_label_col(tripartit_df)

def detect_id_col(df):
    candidates = [c for c in df.columns if c.lower() in ('id','node_id','label_id','idx','index')]
    if candidates:
        return candidates[0]
    for c in df.columns:
        if 'id'==c.lower() or c.lower().endswith('_id'):
            return c
    return None

id_col = detect_id_col(tripartit_df)

def detect_type_col(df):
    candidates = [c for c in df.columns if 'type' in c.lower() or 'category' in c.lower() or 'node_type'==c.lower() or 'class' in c.lower()]
    return candidates[0] if candidates else None

type_col = detect_type_col(tripartit_df)

type_mapping = {
    "dish": "Recept",
    "recipe": "Recept",
    "alapanyag": "Alapanyag",
    "ingredient": "Alapanyag",
    "molecule": "Molekula",
    "molekula": "Molekula",
    "ing": "Alapanyag",
    "food": "Alapanyag"
}

if label_col is None:
    tripartit_df['Label'] = tripartit_df.apply(lambda r: f"node_{r.name}", axis=1)
    label_col = 'Label'

if id_col is None:
    tripartit_df['node_id'] = tripartit_df.index.astype(str).apply(lambda x: f"node_{x}")
    id_col = 'node_id'
else:
    tripartit_df['node_id'] = tripartit_df[id_col].astype(str)

if type_col:
    tripartit_df['_type_raw'] = tripartit_df[type_col].astype(str).fillna("")
    tripartit_df['node_type'] = tripartit_df['_type_raw'].apply(lambda v: type_mapping.get(v.strip().lower(), None) or next((type_mapping.get(tok) for tok in re.split(r'[\s,;/]+', v.strip().lower()) if tok in type_mapping), None) or "Egy√©b")
else:
    tripartit_df['node_type'] = "Egy√©b"

tripartit_df['Label'] = tripartit_df[label_col].astype(str).apply(strip_icon_ligatures)

def normalize_label(s):
    if not isinstance(s, str):
        return ""
    cleaned = strip_icon_ligatures(s)
    cleaned = cleaned.lower()
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    return cleaned

tripartit_df['norm_label'] = tripartit_df['Label'].apply(normalize_label)
tripartit_df['norm_id'] = tripartit_df['node_id'].astype(str).apply(lambda x: normalize_label(str(x)))

node_norm_map = {}
node_id_map = {}
for _, row in tripartit_df.iterrows():
    norm = row['norm_label']
    nid = str(row['node_id'])
    rec = row.to_dict()
    node_norm_map[norm] = rec
    node_id_map[nid] = rec

def find_edge_candidate_cols(edges_df):
    cols = list(edges_df.columns)
    src_candidates = [c for c in cols if 'source' in c.lower() or 'from' in c.lower() or c.lower().startswith('src')]
    tgt_candidates = [c for c in cols if 'target' in c.lower() or 'to' in c.lower() or c.lower().startswith('dst') or c.lower().startswith('tgt')]
    if not src_candidates:
        src_candidates = [c for c in cols if 'label' in c.lower() or 'name' in c.lower()][:2]
    if not tgt_candidates:
        tgt_candidates = [c for c in cols if 'label' in c.lower() or 'name' in c.lower()][:2]
    if not src_candidates:
        src_candidates = cols[:1]
    if not tgt_candidates:
        tgt_candidates = cols[-1:]
    return src_candidates, tgt_candidates

src_candidates, tgt_candidates = find_edge_candidate_cols(edges_df)

def resolve_endpoint_value(val):
    if val is None:
        return ""
    sval = str(val).strip()
    if not sval:
        return ""
    s_norm = normalize_label(sval)
    if s_norm in node_norm_map:
        return s_norm
    if sval in node_id_map:
        return normalize_label(node_id_map[sval].get('Label',''))
    if s_norm in node_id_map:
        return s_norm
    if sval in node_id_map:
        return normalize_label(node_id_map[sval].get('Label',''))
    return s_norm

def compute_edge_norms(edges_df):
    norm_sources = []
    norm_targets = []
    for _, row in edges_df.iterrows():
        src_val = None
        for c in src_candidates:
            if c in row and str(row[c]).strip():
                src_val = row[c]
                break
        tgt_val = None
        for c in tgt_candidates:
            if c in row and str(row[c]).strip():
                tgt_val = row[c]
                break
        src_norm = resolve_endpoint_value(src_val)
        tgt_norm = resolve_endpoint_value(tgt_val)
        norm_sources.append(src_norm)
        norm_targets.append(tgt_norm)
    edges_df = edges_df.copy()
    edges_df['norm_source'] = norm_sources
    edges_df['norm_target'] = norm_targets
    return edges_df

edges_df = compute_edge_norms(edges_df)

all_nodes = tripartit_df.to_dict("records")
all_edges = edges_df.to_dict("records")
historical_recipes = historical_df.to_dict("records")

def load_full_recipe_corpus_from_hist(historical_recipes):
    recipes_full = []
    for recipe in historical_recipes:
        full_text = recipe.get('original_text', '') or ''
        title = strip_icon_ligatures(recipe.get('title', 'N√©vtelen'))
        ingredients = recipe.get('ingredients', '') or ''
        context = f"""
RECEPT C√çM: {title}

ALAPANYAGOK: {ingredients}

TELJES SZ√ñVEG:
{full_text}
        """.strip()
        recipes_full.append({
            'title': title,
            'ingredients': ingredients,
            'full_text': full_text,
            'context': context,
            'word_count': len(full_text.split())
        })
    return recipes_full

full_recipe_corpus = load_full_recipe_corpus_from_hist(historical_recipes)

FASTING_RECIPE_TITLES = {
    "K√°poszta ikr√°val", "Alma-l√©v", "Mondola-perec", "Koldus-l√©v", "√âg-l√©v",
    "Zs√°kv√°szonnal", "Gutta-l√©v", "Sz√≠jalt r√°k", "Lengyel cibre", "K√∂rtv√©ly f≈ëve",
    "Sal√°ta", "Torzsa-sal√°ta", "Ugorka-sal√°ta", "Miskul√°ncia-sal√°ta", "Mondola-l√©v",
    "Bot-l√©v", "Kendermag-cibre", "Ikr√°t f≈ëzni", "Nyers k√°poszta-sal√°ta", "Bors√≥leves",
    "P√°rolt r√°k", "Korpa-cibre", "Bors√≥t f≈ëzni", "Ugork√°t t√©lre s√≥zni", "Feny≈ëgomb√°t f≈ëzni",
    "K√≠nzott k√°sa", "Lencseleves", "Hal rizsk√°s√°val", "Olaj-sp√©k", "Cicer",
    "S√ºlt hal", "L√©mony√°val", "T√∂r√∂tt l√©vel hal", "Csuk√°t csuka-l√©vel", "Olajos domika",
    "Koz√°k-l√©vel", "Z√∂ld l√©vel", "Borsos szilva", "Ecetes cibre", "Hal fekete l√©vel",
    "Zuppon-l√©v", "Tiszta borssal", "Bors-porral", "Viz√°t viza-l√©vel", "Sz√∂m√∂rcs√∂k-gomba",
    "Bor√≠tott l√©v", "K√°sa olajjal", "Lencse olajjal", "Bors√≥ lask√°val", "K√°poszt√°s b√©les",
    "Hagyma r√°ntva", "K√°poszta-l√©v cibre", "L√∂nye", "L√°sa", "S√≥s v√≠z",
    "Seres keny√©r", "Olajos l√©v", "Viza ikra", "√öj k√°poszta"
}

def is_fasting_recipe(recipe):
    title = (recipe.get("title") or "").strip()
    return title in FASTING_RECIPE_TITLES

def create_network_graph(center_node, connected_nodes):
    if not center_node or not connected_nodes:
        return None
    G = nx.Graph()
    G.add_node(center_node, node_type='center')
    for n in connected_nodes:
        G.add_node(n["name"], degree=n.get("degree", 0), node_type=n.get("type", "unknown"))
        G.add_edge(center_node, n["name"], weight=n.get("degree", 1))
    pos = nx.spring_layout(G, k=2.5, iterations=100, seed=42)
    edge_trace = []
    for edge in G.edges():
        x0, y0 = pos[edge[0]]
        x1, y1 = pos[edge[1]]
        edge_trace.append(
            go.Scatter(x=[x0, x1, None], y=[y0, y1, None], mode='lines',
                       line=dict(width=1.5, color='rgba(255,255,255,0.95)'), hoverinfo='none', showlegend=False)
        )
    node_colors = {'center': '#ccaa77', 'Alapanyag': '#8b5a2b', 'Molekula': '#808080', 'Recept': '#800000', 'unknown': '#999999'}
    node_x, node_y, node_text, node_size, node_color = [], [], [], [], []
    max_degree = max([n.get("degree", 1) for n in connected_nodes], default=1)
    for node in G.nodes():
        x, y = pos[node]
        node_x.append(x)
        node_y.append(y)
        if node == center_node:
            node_text.append(f"<b style='font-size: 14px'>{node}</b><br><i>(k√∂zponti)</i>")
            node_size.append(44)
            node_color.append(node_colors['center'])
        else:
            degree = next((n["degree"] for n in connected_nodes if n["name"] == node), 1)
            node_type = next((n.get("type", "unknown") for n in connected_nodes if n["name"] == node), "unknown")
            node_text.append(f"<b>{node}</b><br>Degree: {degree}<br>T√≠pus: {node_type}")
            node_size.append(16 + (degree / max_degree) * 34)
            node_color.append(node_colors.get(node_type, node_colors['unknown']))
    node_trace = go.Scatter(
        x=node_x, y=node_y, mode='markers+text', hovertemplate='%{text}<extra></extra>',
        text=[n.split('<br>')[0].replace('<b style=\'font-size: 14px\'>', '').replace('</b>', '').replace('<b>', '') for n in node_text],
        textposition="top center", textfont=dict(size=10, family="Crimson Text", color='white'),
        marker=dict(size=node_size, color=node_color, line=dict(width=2, color='white')), customdata=node_text, showlegend=False
    )
    fig = go.Figure(data=edge_trace + [node_trace])
    fig.update_layout(showlegend=False, hovermode='closest', margin=dict(b=0, l=0, r=0, t=0),
                      xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                      yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                      paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)', template='plotly_dark', height=800)
    return fig

def build_gpt_context(nodes, recipes, perfect_ings=None, user_query=None, max_nodes=120, max_recipes=40):
    grouped = {}
    for n in nodes:
        grouped.setdefault(n.get("node_type", "Egy√©b"), []).append(n)
    sampled_nodes = []
    if grouped:
        for group in grouped.values():
            sampled_nodes.extend(random.sample(group, min(len(group), max_nodes // max(1, len(grouped)))))
    else:
        sampled_nodes = nodes[:max_nodes]
    normalized_query = None
    if user_query:
        def _normalize(s):
            if not isinstance(s, str):
                return ""
            s = s.lower()
            s = unicodedata.normalize('NFKD', s)
            s = ''.join(ch for ch in s if not unicodedata.combining(ch))
            s = re.sub(r"[^a-z0-9]+", ' ', s)
            s = ' '.join(s.split())
            return s
        normalized_query = _normalize(user_query)
    nodes_ctx = []
    for node in sampled_nodes:
        entry = dict(node)
        entry_name = strip_icon_ligatures(entry.get("Label")
                                          or entry.get("label")
                                          or entry.get("node_name")
                                          or entry.get("node_id")
                                          or entry.get("name")
                                          or "")
        entry["name"] = entry_name
        nodes_ctx.append(entry)

    simplified_nodes = [
        {
            "name": n["name"],
            "type": n.get("node_type") or n.get("type") or n.get("Type") or "Egy√©b",
            "degree": int(n.get("Degree", n.get("degree", 0) or 0))
        }
        for n in nodes_ctx if n.get("name")
    ]

    if normalized_query and user_query:
        q_norm = normalized_query
        q_tokens = [t for t in q_norm.split() if len(t) > 1]
        if q_tokens:
            def _normalize(s):
                if not isinstance(s, str):
                    return ""
                s = s.lower()
                s = unicodedata.normalize('NFKD', s)
                s = ''.join(ch for ch in s if not unicodedata.combining(ch))
                s = re.sub(r"[^a-z0-9]+", ' ', s)
                s = ' '.join(s.split())
                return s
            matched = [n for n in nodes if any(tok in _normalize(n.get("Label", "")) for tok in q_tokens)]
            matched_perfect = []
            if perfect_ings:
                for p in (perfect_ings if isinstance(perfect_ings, list) else [perfect_ings]):
                    label = None
                    if isinstance(p, str):
                        label = p
                    elif isinstance(p, dict):
                        for key in ("label", "name", "ingredient", "term", "alapanyag"):
                            if key in p and isinstance(p[key], str):
                                label = p[key]
                                break
                        if not label:
                            for v in p.values():
                                if isinstance(v, str):
                                    label = v
                                    break
                    if label and any(tok in _normalize(label) for tok in q_tokens):
                        matched_perfect.append({"Label": label, "node_type": "Alapanyag", "Degree": 0})
            seen_labels = {_normalize(n.get("Label", "")) for n in sampled_nodes}
            for m in matched + matched_perfect:
                m_label = _normalize(m.get("Label", ""))
                if m_label and m_label not in seen_labels:
                    sampled_nodes.insert(0, m)
                    seen_labels.add(m_label)
    related_nodes = [n["name"] for n in nodes_ctx if n.get("name")]
    related_analogies = []
    for node in nodes_ctx:
        analogies = HISTORICAL_ANALOGY_MAP.get(node.get("name"))
        if analogies:
            related_analogies.extend(analogies)
    related_analogies = ", ".join(dict.fromkeys(related_analogies))
    system_prompt = f"""
    ...
    Kapcsol√≥d√≥ alapanyagok: {', '.join(related_nodes)}
    Kapcsol√≥d√≥ t√∂rt√©neti anal√≥gi√°k: {related_analogies}
    """
    return nodes_ctx, simplified_nodes

def extract_json_from_text(text: str):
    if not isinstance(text, str):
        return None
    text = text.strip()
    try:
        return json.loads(text)
    except Exception:
        pass
    m = re.search(r"```(?:json)?\s*(\{.*\})\s*```", text, flags=re.S)
    if m:
        try:
            return json.loads(m.group(1))
        except Exception:
            pass
    start = text.find('{')
    end = text.rfind('}')
    if start != -1 and end != -1 and end > start:
        candidate = text[start:end+1]
        try:
            return json.loads(candidate)
        except Exception:
            pass
    return None

def fuzzy_suggest_nodes(query: str, max_suggestions: int = 5):
    if not query:
        return []
    q_norm = normalize_label(query)
    tokens = [t for t in re.split(r'[\s,;:()"\']+', q_norm) if t]
    full_labels = [n.get("Label","") for n in all_nodes if n.get("Label")]
    full_norms = [normalize_label(l) for l in full_labels]
    suggestions = []
    seen = set()
    for tok in tokens:
        if not tok:
            continue
        matches = difflib.get_close_matches(tok, full_norms, n=max_suggestions, cutoff=0.6)
        for m in matches:
            if m not in seen:
                seen.add(m)
                suggestions.append(node_norm_map.get(m, {}).get("Label", m))
    if len(suggestions) < max_suggestions:
        for tok in tokens:
            for i, n in enumerate(full_norms):
                if tok in n and full_labels[i] not in suggestions:
                    suggestions.append(full_labels[i])
                    if len(suggestions) >= max_suggestions:
                        break
            if len(suggestions) >= max_suggestions:
                break
    if len(suggestions) < max_suggestions:
        extra = [l for l in full_labels if l not in suggestions][:max_suggestions - len(suggestions)]
        suggestions.extend(extra)
    return suggestions[:max_suggestions]

def search_recipes_by_query(query: str, max_results: int = 3):
    if not query:
        return []
    q_norm = query.lower()
    q_tokens = [t for t in re.sub(r'[^a-z0-9\s]', ' ', q_norm).split() if len(t) > 1]
    matches = []
    for r in full_recipe_corpus:
        text = (r.get('full_text') or "").lower()
        title = (r.get('title') or "").lower()
        score = 0
        for tok in q_tokens:
            if tok in text:
                score += 2
            if tok in title:
                score += 3
        if score > 0:
            matches.append((score, r))
    matches.sort(key=lambda x: x[0], reverse=True)
    return [ {"title": m[1].get("title",""), "excerpt": (m[1].get("full_text","")[:400])} for m in matches[:max_results] ]

def analyze_query_tokens(user_query: str):
    tokens = [t for t in re.split(r'[\s,;:()"\']+', normalize_label(user_query)) if t]
    analysis = []
    for tok in tokens:
        item = {"token": tok, "base": tok, "role": None, "status": None, "strategy": None, "mapped_to": None, "confidence": 0.0}
        if tok in GENERIC_TOKENS:
            item["role"] = "generic_food"
            item["status"] = "generic"
            item["strategy"] = "ignore_for_node_selection"
            item["confidence"] = 0.2
            analysis.append(item)
            continue
        if tok in ANACHRONISTIC_INGREDIENTS:
            item["role"] = "ingredient"
            item["status"] = "anachronistic"
            mapped = HISTORICAL_ANALOGY_MAP.get(tok)
            if mapped:
                item["mapped_to"] = mapped
                item["strategy"] = "historical_analogy"
                item["confidence"] = 0.6
            else:
                item["mapped_to"] = None
                item["strategy"] = "analogy_required_manual"
                item["confidence"] = 0.3
            analysis.append(item)
            continue
        if tok.endswith('os') or tok.endswith('√≥s') or tok.endswith('es') or tok.endswith('√©s') or tok.endswith('i'):
            base = tok
            if tok.endswith('os') or tok.endswith('√≥s') or tok.endswith('es') or tok.endswith('√©s'):
                base = tok[:-2]
            elif tok.endswith('i') and len(tok) > 3:
                base = tok[:-1]
            item["base"] = base
            item["role"] = "flavour_descriptor"
            item["status"] = "descriptor"
            mapped_label = None
            norm_base = normalize_label(base)
            if norm_base in SYNONYM_MAP:
                for s in SYNONYM_MAP[norm_base]:
                    if normalize_label(s) in node_norm_map:
                        mapped_label = node_norm_map[normalize_label(s)].get("Label")
                        item["mapped_to"] = [mapped_label]
                        item["strategy"] = "synonym_map"
                        item["confidence"] = 0.8
                        break
            if not mapped_label and norm_base in node_norm_map:
                item["mapped_to"] = [node_norm_map[norm_base].get("Label")]
                item["strategy"] = "direct_node_match"
                item["confidence"] = 0.85
            if not item.get("mapped_to"):
                analogs = HISTORICAL_ANALOGY_MAP.get(norm_base) or HISTORICAL_ANALOGY_MAP.get(tok)
                if analogs:
                    item["mapped_to"] = analogs
                    item["strategy"] = "historical_analogy_for_descriptor"
                    item["confidence"] = 0.55
                else:
                    fuzzy = fuzzy_suggest_nodes(base, max_suggestions=1)
                    if fuzzy:
                        item["mapped_to"] = fuzzy
                        item["strategy"] = "fuzzy_fallback"
                        item["confidence"] = 0.4
                    else:
                        item["mapped_to"] = None
                        item["strategy"] = "no_mapping"
                        item["confidence"] = 0.25
            analysis.append(item)
            continue
        norm_tok = normalize_label(tok)
        if norm_tok in SYNONYM_MAP:
            for s in SYNONYM_MAP[norm_tok]:
                if normalize_label(s) in node_norm_map:
                    item["role"] = "ingredient"
                    item["status"] = "direct_synonym"
                    item["mapped_to"] = [node_norm_map[normalize_label(s)].get("Label")]
                    item["strategy"] = "synonym_map"
                    item["confidence"] = 0.9
                    break
            if item["mapped_to"]:
                analysis.append(item)
                continue
        if norm_tok in node_norm_map:
            item["role"] = "ingredient"
            item["status"] = "direct_node"
            item["mapped_to"] = [node_norm_map[norm_tok].get("Label")]
            item["strategy"] = "direct_node_match"
            item["confidence"] = 0.95
            analysis.append(item)
            continue
        close = difflib.get_close_matches(norm_tok, list(node_norm_map.keys()), n=1, cutoff=0.75)
        if close:
            item["role"] = "ingredient"
            item["status"] = "close_match"
            item["mapped_to"] = [node_norm_map[close[0]].get("Label")]
            item["strategy"] = "close_string_match"
            item["confidence"] = 0.75
            analysis.append(item)
            continue
        if 'bors' in norm_tok or 'pepper' in norm_tok or 'pink' in norm_tok:
            b_candidates = [k for k in node_norm_map.keys() if 'bors' in k or 'pepper' in k or 'tiszta borssal' in k or 'r√≥zsabors' in k]
            if b_candidates:
                cand = difflib.get_close_matches(norm_tok, b_candidates, n=1, cutoff=0.35)
                if cand:
                    item["role"] = "ingredient"
                    item["status"] = "pepper_family"
                    item["mapped_to"] = [node_norm_map[cand[0]].get("Label")]
                    item["strategy"] = "special_pepper_rules"
                    item["confidence"] = 0.85
                    analysis.append(item)
                    continue
        fuzzy = fuzzy_suggest_nodes(tok, max_suggestions=1)
        if fuzzy:
            item["role"] = "ingredient"
            item["status"] = "fuzzy_suggest"
            item["mapped_to"] = fuzzy
            item["strategy"] = "fuzzy"
            item["confidence"] = 0.35
            analysis.append(item)
            continue
        item["role"] = "unknown"
        item["status"] = "no_mapping"
        item["strategy"] = "no_mapping"
        item["confidence"] = 0.0
        analysis.append(item)
        if tok in HISTORICAL_DISH_STRUCTURE_MAP:
            item["role"] = "dish_structure"
            item["status"] = "historically_interpretable"
            item["mapped_to"] = HISTORICAL_DISH_STRUCTURE_MAP[tok]["historical_equivalents"]
            item["strategy"] = "source_based_structural_mapping"
            item["confidence"] = HISTORICAL_DISH_STRUCTURE_MAP[tok]["confidence"]
            analysis.append(item)
            continue
    return analysis

def build_reasoning_paragraph(token_analysis: list) -> str:
    """
    A token-anal√≠zisb≈ël foly√≥ sz√∂veges, narrat√≠v reasoning-et k√©sz√≠t.
    """
    sentences = []

    for item in token_analysis:
        tok = item["token"]
        role = item["role"]
        status = item["status"]
        strategy = item["strategy"]
        mapped = item.get("mapped_to")

        if role == "flavour_descriptor":
            s = f"A ‚Äû{tok}‚Äù kifejez√©s √≠zle√≠r√≥k√©nt jelenik meg, amely nem √∂n√°ll√≥ alapanyagot, hanem √©rz√©kszervi ir√°nyt jel√∂l."
        elif status == "anachronistic":
            s = f"A ‚Äû{tok}‚Äù modern alapanyagnak sz√°m√≠t, ez√©rt t√∂rt√©neti anal√≥gi√°val ker√ºlt √©rtelmez√©sre."
        elif strategy == "historical_analogy" and mapped:
            s = f"A ‚Äû{tok}‚Äù eset√©ben a t√∂rt√©neti forr√°sok alapj√°n a k√∂vetkez≈ë anal√≥g √∂sszetev≈ëk j√∂hetnek sz√≥ba: {', '.join(mapped)}."
        elif strategy == "direct_node_match":
            s = f"A ‚Äû{tok}‚Äù egy√©rtelm≈±en azonos√≠that√≥ a t√∂rt√©neti adatb√°zisban szerepl≈ë alapanyagk√©nt."
        elif strategy == "fuzzy_fallback":
            s = f"A ‚Äû{tok}‚Äù pontos megfelel≈ëje nem szerepel az adatb√°zisban, ez√©rt hangalaki hasonl√≥s√°g alapj√°n t√∂rt√©nt becsl√©s."
        else:
            s = f"A ‚Äû{tok}‚Äù √©rtelmez√©se bizonytalan, ez√©rt csak korl√°tozottan befoly√°solta a keres√©st."

        sentences.append(s)

    return " ".join(sentences)

def gpt_search_recipes(user_query):
    query_lower = (user_query or "").strip()
    matched_recipes = []
    if query_lower:
        q_tokens = [t for t in re.sub(r'[^a-z0-9\s]', ' ', query_lower.lower()).split() if len(t) > 1]
    else:
        q_tokens = []
    for recipe in full_recipe_corpus:
        text = (recipe.get('full_text') or "").lower()
        if not text:
            continue
        if q_tokens and any(tok in text for tok in q_tokens):
            matched_recipes.append(recipe)
            if len(matched_recipes) >= 10:
                break
    nodes_ctx, simplified_nodes = build_gpt_context(all_nodes, historical_recipes, perfect_ings, user_query=query)
    system_prompt = f"""
    Te egy XVII. sz√°zadi magyar szak√°csk√∂nyv st√≠lus√°ban √≠rsz AI Aj√°nl√°st.
    Feladat: a felhaszn√°l√≥i kifejez√©seket essz√©szer≈±en √©rtelmezd, kultur√°lis √©s √©rz√©ki szempontokat √∂sszekapcsolva.
    Ne list√°zz, hanem foly√©kony pr√≥z√°ban indokold, mi√©rt √©s hogyan √©rtelmezted a szavakat t√∂rt√©neti gasztron√≥miai logika ment√©n.
    A c√©l: az √≠z√©lm√©ny, text√∫ra √©s jelent√©s t√∂rt√©neti rekonstrukci√≥ja.
    Felhaszn√°l√≥i query: {user_query}
    Kapcsol√≥d√≥ alapanyagok: {', '.join([n['name'] for n in simplified_nodes])}    node_analogies = []
    for node in simplified_nodes:
        node_analogies.extend(HISTORICAL_ANALOGY_MAP.get(node["name"], []))
    related_analogies = ", ".join(dict.fromkeys(node_analogies))
    """
    top_matched = matched_recipes[:5]
    matched_preview = [{"title": r.get("title", ""), "excerpt": (r.get("full_text") or "")[:400]} for r in top_matched]
    try:
        full_labels = sorted({n.get("Label", "") for n in all_nodes if n.get("Label")})
        full_labels_preview = json.dumps(full_labels[:300], ensure_ascii=False)
    except Exception:
        full_labels_preview = "[]"
    try:
        perfect_preview = (json.dumps(perfect_ings[:50], ensure_ascii=False) if isinstance(perfect_ings, list) else json.dumps(perfect_ings, ensure_ascii=False))
    except Exception:
        perfect_preview = "[]"
    user_prompt = f"""
Nyelv: magyar

Felhaszn√°l√≥i lek√©rdez√©s: "{user_query}"

El√©rhet≈ë csom√≥pontok (r√∂vid mintav√©tel):
{json.dumps(nodes_ctx[:40], ensure_ascii=False)}

Tal√°lhat√≥ t√∂rt√©neti recept-r√©szletek:
{json.dumps(matched_preview, ensure_ascii=False)}

Teljes node-c√≠mek (r√∂vid el≈ën√©zet):
{full_labels_preview}

T√∂k√©letes alapanyagok (r√∂vid):
{perfect_preview}

Utas√≠t√°sok: system_prompt = """
El≈ësz√∂r foly√≥, magyar (vagy a felhaszn√°l√≥ √°ltal √≠rt b√°rmilyen nyelven) nyelv≈± magyar√°z√≥ sz√∂vegben √≠rd le, hogyan √©rtelmezed a felhaszn√°l√≥ k√©rd√©s√©t t√∂rt√©neti-gasztron√≥miai szempontb√≥l. Ezut√°n ‚Äì k√ºl√∂n blokkban ‚Äì add meg a struktur√°lt adatokat JSON form√°tumban. A sz√∂veg legyen √©lvezetes, √©rtelmez≈ë jelleg≈±, ne csak felsorol√°s. Ha a felhaszn√°l√≥ olyan kifejez√©st eml√≠t, amely nincs a node-list√°ban, t√©rk√©pezd a legk√∂zelebbi ismert node-ra √©s r√©szletezd a mapping indokl√°s√°t a "reasoning" mez≈ëben. Javasolj legfeljebb 5 node-ot √©s legfeljebb 3 t√∂rt√©neti receptc√≠meket.
"""
    try:
        response = client.responses.create(model="gpt-5.1", input=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}], max_output_tokens=900)
        raw = response.output_text if hasattr(response, "output_text") else (response.get("output_text") if isinstance(response, dict) else str(response))
        parsed = extract_json_from_text(raw)
        if parsed and isinstance(parsed, dict):
            if "suggested_nodes" in parsed and "suggested_recipes" in parsed:
                return parsed
        raise ValueError("Invalid JSON from model")
    except Exception:
        suggested_nodes = fuzzy_suggest_nodes(user_query, max_suggestions=5)
        suggested_recipes = [r["title"] for r in search_recipes_by_query(user_query, max_results=3)]
        analysis = analyze_query_tokens(user_query)
        reasoning_parts = []
        mapped_nodes = []
        for item in analysis:
            tok = item["token"]
            status = item.get("status", "unknown")
            strat = item.get("strategy", "none")
            conf = item.get("confidence", 0.0)
            mapped = item.get("mapped_to")
            if isinstance(mapped, list):
                mapped_display = ", ".join([str(m) for m in mapped if m])
            else:
                mapped_display = str(mapped) if mapped else "‚Äî"
            reasoning_parts.append(f'"{tok}" ‚Üí st√°tusz: {status}; strat√©gia: {strat}; lek√©pez√©s: {mapped_display}; bizalom: {conf:.2f}')
            if item.get("mapped_to"):
                if isinstance(item["mapped_to"], list):
                    for m in item["mapped_to"]:
                        if isinstance(m, str) and normalize_label(m) in node_norm_map:
                            mapped_nodes.append(node_norm_map[normalize_label(m)].get("Label"))
                        elif isinstance(m, str):
                            mapped_nodes.append(m)
                else:
                    m = item["mapped_to"]
                    if isinstance(m, str) and normalize_label(m) in node_norm_map:
                        mapped_nodes.append(node_norm_map[normalize_label(m)].get("Label"))
                    elif isinstance(m, str):
                        mapped_nodes.append(m)
        mapped_nodes = [m for m in mapped_nodes if m]
        combined_suggestions = []
        seen = set()
        for n in mapped_nodes + suggested_nodes:
            if n and n not in seen:
                combined_suggestions.append(n)
                seen.add(n)
            if len(combined_suggestions) >= 5:
                break
        if not combined_suggestions:
            combined_suggestions = suggested_nodes[:5]
        analysis = analyze_query_tokens(user_query)
        reasoning_text = build_reasoning_paragraph(analysis)
        result = {
            "suggested_nodes": combined_suggestions,
            "suggested_recipes": suggested_recipes,
            "reasoning": reasoning,
            "mapping": analysis
        }
        return result

def max_similarity_to_historical(candidate: str, historical_list: list) -> float:
    if not candidate or not historical_list:
        return 0.0
    candidate_norm = re.sub(r'\s+', ' ', candidate.strip().lower())
    max_sim = 0.0
    for h in historical_list:
        text = ""
        if isinstance(h, dict):
            text = h.get("text", "") or h.get("original_text", "") or h.get("excerpt", "") or h.get("title", "")
        else:
            text = str(h)
        text_norm = re.sub(r'\s+', ' ', strip_icon_ligatures(text).strip().lower())
        if not text_norm:
            continue
        sim = difflib.SequenceMatcher(None, candidate_norm, text_norm).ratio()
        if sim > max_sim:
            max_sim = sim
    return float(max_sim)

def generate_ai_recipe(selected, connected, historical, user_query=None, samples=4, temperature=0.7):
    system_prompt = """
√çrj egy XVII. sz√°zadi magyar st√≠lus√∫, v√°laszt√©kos √©s besz√©des receptet. Szab√°lyok:
- 70‚Äì110 sz√≥ k√∂z√∂tt
- archaikus, m√©gis √©rthet≈ë magyar st√≠lus, √∂sszetett mondatokkal √©s gazdag sz√≥kinccsel
- haszn√°lj lehet≈ëleg csak a megadott √∂sszetev≈ëket/kapcsolatokat; ha a felhaszn√°l√≥i lek√©rdez√©s modern kifejez√©st tartalmaz, t√©rk√©pezd historikus megfelel≈ëre √©s indokold r√∂viden
- ker√ºld az adott t√∂rt√©neti p√©ld√°k sz√≥ szerinti m√°sol√°s√°t; ha a gener√°lt sz√∂veg >60% hasonl√≥s√°got mutat egy t√∂rt√©neti p√©ld√°hoz, gener√°lj √∫jat
- a v√°lasz CSAK √âS KIZ√ÅR√ìLAG √©rv√©nyes JSON legyen magyar mez≈ënevekkel: legal√°bb 'title', 'archaic_recipe', 'confidence', 'novelty_score', 'word_count'
- legy√©l gondolkod√≥ √©s okos: a 'reasoning' mez≈ëben r√∂viden √≠rd le, hogyan k√©pzeled el a mappingot, ha volt
"""
    user_prompt = f"""
Felhaszn√°l√≥i keres√©s: {user_query}

K√∂zponti elem: {selected}

Kapcsol√≥d√≥ elemek (name,type,degree):
{json.dumps(connected, ensure_ascii=False)}

T√∂rt√©neti p√©ld√°k (r√∂vid):
{json.dumps(historical, ensure_ascii=False)}

Ha valamelyik kapcsolt elem bizonytalan, t√©rk√©pezd a legplausibilisebb t√∂rt√©neti alapanyagra. Adj vissza csak JSON-t.
"""
    candidates = []
    raw_texts = []
    for i in range(samples):
        try:
            response = client.responses.create(model="gpt-5.1", input=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}], temperature=temperature, max_output_tokens=700)
            ai_text = response.output_text.strip() if hasattr(response, "output_text") else str(response)
            parsed = extract_json_from_text(ai_text)
            if parsed and isinstance(parsed, dict):
                candidates.append(parsed)
                raw_texts.append(parsed.get("archaic_recipe", "") or parsed.get("text", "") or "")
            else:
                if ai_text:
                    raw_texts.append(ai_text)
        except Exception:
            continue
    if not candidates and not raw_texts:
        return {"title": "Hiba t√∂rt√©nt", "archaic_recipe": "A recept gener√°l√°sa sikertelen volt: nincs √©rv√©nyes v√°lasz.", "confidence": "low", "word_count": 0, "novelty_score": 0.0}
    hist_texts = []
    for h in historical:
        if isinstance(h, dict):
            hist_texts.append(h.get("text", "") or h.get("original_text", "") or h.get("excerpt", "") or h.get("title", ""))
        else:
            hist_texts.append(str(h))
    best = None
    best_novelty = -1.0
    for cand in candidates:
        recipe_text = cand.get("archaic_recipe", "") or cand.get("text", "") or ""
        sim = max_similarity_to_historical(recipe_text, hist_texts)
        novelty = 1.0 - sim
        cand["novelty_score"] = round(novelty, 4)
        wc = len(recipe_text.split())
        cand["word_count"] = wc
        if 70 <= wc <= 110:
            cand["confidence"] = "high"
        elif 50 <= wc <= 130:
            cand["confidence"] = "medium"
        else:
            cand["confidence"] = "low"
        if novelty > best_novelty:
            best_novelty = novelty
            best = cand
    if not best:
        fallback_text = raw_texts[0] if raw_texts else ""
        wc = len(fallback_text.split())
        return {"title": selected, "archaic_recipe": fallback_text, "confidence": "low", "word_count": wc, "novelty_score": 0.0}
    return best

banner_path = "83076027-f357-4e82-8716-933911048498.png"
if os.path.exists(banner_path):
    with open(banner_path, "rb") as f:
        img_data = base64.b64encode(f.read()).decode()
    st.markdown(f"""
    <div style="position: relative; text-align: center; margin-bottom: 3rem; border-radius: 16px; overflow: hidden; box-shadow: 0 8px 16px rgba(0, 0, 0, 0.7); height: 300px;">
        <img src="data:image/png;base64,{img_data}" style="width: 100%; height: 300px; object-fit: cover; display: block;">
        <div style="position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%); width: 100%; z-index: 10;">
            <h1 style="font-size: 3rem; color: white; text-shadow: 3px 3px 10px black, 0 0 20px rgba(0,0,0,0.8); margin: 0; font-family: 'Cinzel', serif;">
                K√∂zrendek √çzh√°l√≥ja
            </h1>
            <p style="font-size: 1.3rem; font-style: italic; color: white; text-shadow: 2px 2px 8px black, 0 0 15px rgba(0,0,0,0.8); margin-top: 0.5rem; font-family: 'Crimson Text', serif;">
                Fedezd fel a XVII. sz√°zadi magyar konyha √≠zh√°l√≥zat√°t
            </p>
        </div>
    </div>
    """, unsafe_allow_html=True)
else:
    st.markdown("""
    <div style="background: linear-gradient(135deg, #800000 0%, #2d2d2d 50%, #1a1a1a 100%); 
                padding: 2.5rem 2rem; 
                border-radius: 16px; 
                box-shadow: 0 8px 16px rgba(0, 0, 0, 0.7);
                margin-bottom: 2rem;
                border: 3px solid #ccaa77;">
        <h1 style="font-size: 3rem; color: white; text-shadow: 3px 3px 8px black; margin: 0; text-align: center;">
            K√∂zrendek √çzh√°l√≥ja
        </h1>
        <p style="font-size: 1.3rem; font-style: italic; color: #e8dcc8; text-shadow: 2px 2px 6px black; margin-top: 0.5rem; text-align: center;">
            Fedezd fel a XVII. sz√°zadi magyar konyha √≠zh√°l√≥zat√°t
        </p>
    </div>
    """, unsafe_allow_html=True)

st.markdown("""
<div style="background: linear-gradient(135deg, #2d2d2d 0%, #1a1a1a 100%); border: 3px solid #ccaa77; border-radius: 12px; padding: 2rem; margin: 2rem 0; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.5);">
    <h3 style="color: #ccaa77; font-family: 'Cinzel', serif; margin-bottom: 1rem; text-align: center;">
        üîç Intelligens Keres√©s
    </h3>
    <p style="text-align: center; color: #e8dcc8; font-family: 'Crimson Text', serif; font-style: italic; margin-bottom: 1.5rem;">
        √çrj le egy √©telt vagy alapanyagot, √©s az AI megtal√°lja a kapcsol√≥d√≥ node-okat √©s t√∂rt√©neti recepteket!
    </p>
</div>
""", unsafe_allow_html=True)

cols = st.columns(4)
data = [
    {"title": "Csom√≥pontok / Nodes", "value": str(len(all_nodes)), "desc": "Minden egyes node egy alapanyagot, molekul√°t vagy receptet jel√∂l a h√°l√≥zatban; ezek alkotj√°k az √∂sszef√ºgg≈ë √≠zh√°l√≥zat v√°z√°t."},
    {"title": "√âlek / Edges", "value": str(len(all_edges)), "desc": "A kapcsolatok az √∂sszef√ºgg√©seket mutatj√°k: ki milyen alapanyaggal, molekul√°val vagy recepttel van √∂sszek√∂tve."},
    {"title": "Receptek", "value": str(len(historical_recipes)), "desc": "T√∂rt√©neti receptek sz√°ma; ezek adnak kulcsot a node-ok jelent√©s√©hez a XVII. sz√°zadi kontextusban."},
    {"title": "√Åtlag Foksz√°m / Degree", "value": f"{(sum([int(n.get('Degree', 0) or 0) for n in all_nodes]) / max(len(all_nodes),1)):.1f}", "desc": "Az √°tlag foksz√°m azt mutatja, mennyi kapcsolat jut egy csom√≥pontra ‚Äî a magasabb √©rt√©k gazdagabb h√°l√≥zati integr√°ci√≥t jelent."}
]
for col, info in zip(cols, data):
    with col:
        st.markdown(f"""
        <div class="carousell-card">
            <div class="card-title">{info["title"]}</div>
            <div class="card-value">{info["value"]}</div>
            <div class="card-desc">{info["desc"]}</div>
        </div>
        """, unsafe_allow_html=True)

st.markdown("<div style='height: 1cm;'></div>", unsafe_allow_html=True)

col_search, col_sort = st.columns([3, 1])
with col_search:
    query = st.text_input("Keres√©s", placeholder="üîç pl. 'r√≥zsabors', '√©des s√ºtem√©ny mandul√°val', 'boros leves'...", key="search_input", label_visibility="collapsed")

    if query and st.button("ü§ñ AI Keres√©s", key="gpt_search"):
        if "gpt_search_results" in st.session_state:
            del st.session_state["gpt_search_results"]
        if "selected" in st.session_state:
            del st.session_state["selected"]
        if "connected" in st.session_state:
            del st.session_state["connected"]
        if "historical_recipe" in st.session_state:
            del st.session_state["historical_recipe"]
        if "ai_recipe" in st.session_state:
            del st.session_state["ai_recipe"]
        with st.spinner("üîç AI elemzi a k√©r√©st..."):
            search_results = gpt_search_recipes(query)
            st.session_state["gpt_search_results"] = search_results
            st.session_state["search_query"] = query
            try:
                suggested = search_results.get("suggested_nodes", []) or []
                if suggested:
                    top_name = str(suggested[0])
                    top_norm = normalize_label(top_name)
                    node_obj = node_norm_map.get(top_norm)
                    if not node_obj:
                        possible = fuzzy_suggest_nodes(top_name, max_suggestions=1)
                        node_label = possible[0] if possible else top_name
                        node_obj = node_norm_map.get(normalize_label(node_label))
                    if node_obj:
                        sel = node_obj.get("Label")
                        sel_norm = normalize_label(sel)
                        related_norms = []
                        for e in all_edges:
                            es = e.get("norm_source", "")
                            et = e.get("norm_target", "")
                            if sel_norm and es == sel_norm:
                                related_norms.append(et)
                            elif sel_norm and et == sel_norm:
                                related_norms.append(es)
                        related_norms = set([r for r in related_norms if r])
                        connected = []
                        for rn in related_norms:
                            node = node_norm_map.get(rn)
                            if node:
                                connected.append({"name": node.get("Label"), "degree": int(node.get("Degree", 0) or 0), "type": node.get("node_type", "unknown")})
                        historical_recipe = [{"title": strip_icon_ligatures(r.get("title", "N√©vtelen")), "text": strip_icon_ligatures(r.get("original_text", "")[:300])} for r in historical_recipes if sel.lower() in str(r).lower()][:5]
                        st.session_state["selected"] = sel
                        st.session_state["connected"] = connected
                        st.session_state["historical_recipe"] = historical_recipe
                        with st.spinner("‚è≥ AI receptgener√°l√°s..."):
                            ai_recipe = generate_ai_recipe(sel, connected, historical_recipe, user_query=query)
                            st.session_state["ai_recipe"] = ai_recipe
            except Exception:
                pass

if "sort_option" not in st.session_state:
    st.session_state.sort_option = "üìù N√©v (A‚ÄìZ)"

OPTIONS = ["üìù N√©v (A‚ÄìZ)","üîÅ N√©v (Z‚ÄìA)","üìä Degree ‚Üì","üìà Degree ‚Üë"]
if "sort_mode" not in st.session_state:
    st.session_state.sort_mode = "name_asc"
with col_sort:
    st.markdown("#### Rendez√©s")
    c1, c2 = st.columns(2)
    c3, c4 = st.columns(2)
    with c1:
        if st.button("üìù N√©v A‚ÄìZ", use_container_width=True):
            st.session_state.sort_mode = "name_asc"
    with c2:
        if st.button("üîÅ N√©v Z‚ÄìA", use_container_width=True):
            st.session_state.sort_mode = "name_desc"
    with c3:
        if st.button("üìä Degree ‚Üì", use_container_width=True):
            st.session_state.sort_mode = "deg_desc"
    with c4:
        if st.button("üìà Degree ‚Üë", use_container_width=True):
            st.session_state.sort_mode = "deg_asc"

def _node_type(n):
    if not isinstance(n, dict):
        return "Egy√©b"
    return n.get("node_type") or n.get("Type") or n.get("type") or "Egy√©b"

def _node_label(n):
    if not isinstance(n, dict):
        return ""
    return strip_icon_ligatures(n.get("Label") or n.get("label") or "")

def _node_degree(n):
    try:
        return int(n.get("Degree", n.get("degree", 0) or 0))
    except Exception:
        return 0

node_types = sorted({ _node_type(n) for n in all_nodes if isinstance(n, dict) })
label_map = {t: f"üß± {t}" if t=="Alapanyag" else ("üß™ "+t if t=="Molekula" else ("üìñ "+t if t=="Recept" else t)) for t in node_types}
choices = [label_map[t] for t in node_types]
node_type_filter = st.multiselect("Kateg√≥ria", options=node_types, default=node_types, key="node_type_filter", help="Sz≈±r√©s csom√≥pont-t√≠pus szerint")
node_type_filter_set = set(node_type_filter) if node_type_filter else set(node_types)
filtered_nodes = []

if "gpt_search_results" not in st.session_state or not query:
    candidates = (all_nodes or [])
else:
    suggested = st.session_state["gpt_search_results"].get("suggested_nodes", [])
    candidates = []
    for n in (all_nodes or []):
        if not isinstance(n, dict):
            continue
        if _node_type(n) not in node_type_filter_set:
            continue
        label = n.get("Label", "")
        if not query or query.lower() in str(label).lower() or label in suggested:
            candidates.append(n)

if "gpt_search_results" not in st.session_state or not query:
    for n in candidates:
        if not isinstance(n, dict):
            continue
        if _node_type(n) in node_type_filter_set:
            label = n.get("Label", "")
            if not query or query.lower() in str(label).lower():
                filtered_nodes.append(n)
else:
    filtered_nodes = candidates

mode = st.session_state.sort_mode
if mode == "name_asc":
    filtered_nodes.sort(key=lambda x: _node_label(x).lower())
elif mode == "name_desc":
    filtered_nodes.sort(key=lambda x: _node_label(x).lower(), reverse=True)
elif mode == "deg_desc":
    filtered_nodes.sort(key=lambda x: _node_degree(x), reverse=True)
elif mode == "deg_asc":
    filtered_nodes.sort(key=lambda x: _node_degree(x))

cols = st.columns(6)
for i, n in enumerate(filtered_nodes[:60]):
    type_emoji = {'Alapanyag': 'üß±', 'Molekula': 'üß™', 'Recept': 'üìñ', 'Egy√©b': '‚ö™'}.get(n.get('node_type'), '‚ö™')
    clean_label = strip_icon_ligatures(n.get('Label', ''))
    if cols[i % 6].button(f"{type_emoji} {clean_label}", key=f"node_{i}"):
        sel = n.get("Label", "")
        sel_norm = normalize_label(sel)
        related_norms = []
        for e in all_edges:
            es = e.get("norm_source", "")
            et = e.get("norm_target", "")
            if sel_norm and es == sel_norm:
                related_norms.append(et)
            elif sel_norm and et == sel_norm:
                related_norms.append(es)
        related_norms = set([r for r in related_norms if r])
        connected = []
        for rn in related_norms:
            node = node_norm_map.get(rn)
            if node:
                connected.append({"name": node.get("Label"), "degree": int(node.get("Degree", 0) or 0), "type": node.get("node_type", "unknown")})
        historical_recipe = [{"title": strip_icon_ligatures(r.get("title", "N√©vtelen")), "text": strip_icon_ligatures(r.get("original_text", "")[:300])} for r in historical_recipes if sel.lower() in str(r).lower()][:5]
        st.session_state["selected"] = sel
        st.session_state["connected"] = connected
        st.session_state["historical_recipe"] = historical_recipe
        with st.spinner("‚è≥ AI receptgener√°l√°s..."):
            ai_recipe = generate_ai_recipe(sel, connected, historical_recipe, user_query=st.session_state.get("search_query"))
            st.session_state["ai_recipe"] = ai_recipe
        st.rerun()

if "gpt_search_results" in st.session_state:
    results = st.session_state["gpt_search_results"]
    reasoning = strip_icon_ligatures(results.get('reasoning', ''))
    st.markdown(f"""
    <div style="background: linear-gradient(135deg, #2d2d2d, #1a1a1a); border: 2px solid #ccaa77; border-radius: 12px; padding: 1.5rem; margin: 1rem 0;">
        <h4 style="color: #ccaa77; font-family: 'Cinzel', serif; margin-bottom: 0.5rem;">
            üí° AI Aj√°nl√°s: "{strip_icon_ligatures(st.session_state.get('search_query', ''))}"
        </h4>
        <p style="color: #e8dcc8; font-family: 'Crimson Text', serif; font-style: italic;">
            {reasoning}
        </p>
    </div>
    """, unsafe_allow_html=True)
    if results.get("suggested_nodes"):
        st.markdown("**üéØ Aj√°nlott alapanyagok/csom√≥pontok (nodes):**")
        cols_suggested = st.columns(min(len(results["suggested_nodes"]), 5))
        for i, node_name in enumerate(results["suggested_nodes"][:5]):
            clean_node_name = strip_icon_ligatures(str(node_name))
            node = node_norm_map.get(normalize_label(clean_node_name))
            if not node:
                poss = fuzzy_suggest_nodes(clean_node_name, max_suggestions=1)
                if poss:
                    node = node_norm_map.get(normalize_label(poss[0]))
            if node and i < len(cols_suggested):
                type_emoji = {'Alapanyag': 'üß±', 'Molekula': 'üß™', 'Recept': 'üìñ', 'Egy√©b': '‚ö™'}.get(node.get('node_type'), '‚ö™')
                clean_label = strip_icon_ligatures(node.get('Label', ''))
                if cols_suggested[i].button(f"{type_emoji} {clean_label}", key=f"suggested_{i}"):
                    sel = node.get("Label", "")
                    sel_norm = normalize_label(sel)
                    related_norms = []
                    for e in all_edges:
                        es = e.get("norm_source", "")
                        et = e.get("norm_target", "")
                        if sel_norm and es == sel_norm:
                            related_norms.append(et)
                        elif sel_norm and et == sel_norm:
                            related_norms.append(es)
                    related_norms = set([r for r in related_norms if r])
                    connected = []
                    for rn in related_norms:
                        nnode = node_norm_map.get(rn)
                        if nnode:
                            connected.append({"name": nnode.get("Label"), "degree": int(nnode.get("Degree", 0) or 0), "type": nnode.get("node_type", "unknown")})
                    historical_recipe = [{"title": strip_icon_ligatures(r.get("title", "N√©vtelen")), "text": strip_icon_ligatures(r.get("original_text", "")[:300])} for r in historical_recipes if sel.lower() in str(r).lower()][:5]
                    st.session_state["selected"] = sel
                    st.session_state["connected"] = connected
                    st.session_state["historical_recipe"] = historical_recipe
                    with st.spinner("‚è≥ AI receptgener√°l√°s..."):
                        ai_recipe = generate_ai_recipe(sel, connected, historical_recipe, user_query=st.session_state.get("search_query"))
                        st.session_state["ai_recipe"] = ai_recipe
                    st.rerun()
    if results.get("suggested_recipes"):
        st.markdown("**üìñ Relev√°ns t√∂rt√©neti receptek:**")
        for recipe_title in results["suggested_recipes"][:3]:
            clean_recipe_title = strip_icon_ligatures(str(recipe_title))
            recipe = next((r for r in historical_recipes if strip_icon_ligatures(r.get("title", "")).lower() == clean_recipe_title.lower()), None)
            if recipe:
                clean_title = strip_icon_ligatures(recipe.get('title', 'N√©vtelen'))
                clean_text = strip_icon_ligatures(recipe.get('original_text', '')[:400])
                with st.expander(f"üìú {clean_title}"):
                    st.markdown(clean_text + "...")

if "selected" in st.session_state:
    st.markdown("---")
    st.markdown(f"<h2 style='text-align: center;'>üéØ {strip_icon_ligatures(st.session_state['selected'])}</h2>", unsafe_allow_html=True)
    st.markdown("### üó∫Ô∏è H√°l√≥zati T√©rk√©p")
    fig = create_network_graph(st.session_state["selected"], st.session_state["connected"])
    if fig:
        st.plotly_chart(fig, use_container_width=True)
    st.markdown("<br>", unsafe_allow_html=True)
    col1, col2 = st.columns([1, 1])
    with col1:
        st.markdown("### üìö T√∂rt√©neti P√©ld√°k")
        recipe = st.session_state.get("historical_recipe", [])
        if recipe:
            for ex in recipe[:3]:
                clean_title = strip_icon_ligatures(ex.get('title', 'N√©vtelen'))
                clean_text = strip_icon_ligatures(ex.get('text', ''))
                with st.expander(f"üìñ {clean_title}"):
                    st.markdown(clean_text)
        else:
            st.info("Nincs t√∂rt√©neti p√©lda")
    with col2:
        st.markdown("### ü§ñ AI Gener√°lt Recept")
        ai_recipe = st.session_state.get("ai_recipe")
        if ai_recipe:
            clean_ai_title = strip_icon_ligatures(ai_recipe.get('title', 'C√≠m n√©lk√ºl'))
            clean_ai_text = strip_icon_ligatures(ai_recipe.get('archaic_recipe', ''))
            st.markdown(f"""
            <div style="background: linear-gradient(135deg, #2d2d2d 0%, #1a1a1a 100%); border: 3px solid #ccaa77; border-radius: 12px; padding: 2rem; box-shadow: 0 6px 12px rgba(0, 0, 0, 0.5);">
                <h3 style="color: #ccaa77; font-family: 'Cinzel', serif; margin-bottom: 1rem;">{clean_ai_title}</h3>
                <p style="color: #e8dcc8; font-family: 'Crimson Text', serif; line-height: 1.8; font-size: 1.1rem;">{clean_ai_text}</p>
                <div style="display: flex; gap: 1rem; margin-top: 1.5rem;">
                    <span style="background: #800000; padding: 0.6rem 1rem; border-radius: 8px; color: #ccaa77; font-weight: 600;">‚úì {ai_recipe.get('confidence', 'unknown')}</span>
                    <span style="background: #800000; padding: 0.6rem 1rem; border-radius: 8px; color: #ccaa77; font-weight: 600;">üìù {ai_recipe.get('word_count', 0)} sz√≥</span>
                    <span style="background: #800000; padding: 0.6rem 1rem; border-radius: 8px; color: #ccaa77; font-weight: 600;">‚ú® {int(ai_recipe.get('novelty_score', 0.0)*100)}% √∫j</span>
                </div>
            </div>
            """, unsafe_allow_html=True)
        else:
            st.error("‚ùå Hiba t√∂rt√©nt a gener√°l√°s sor√°n")

st.markdown("---")
st.markdown("""
<div style="text-align: center; margin: 3rem 0 2rem 0;">
    <h3 style="color: #ccaa77; font-family: 'Cinzel', serif; margin-bottom: 1.5rem;">
        üß≠ Tov√°bbi oldalak
    </h3>
</div>
""", unsafe_allow_html=True)

nav_col1, nav_col2 = st.columns(2)
with nav_col1:
    st.markdown("""
    <div style="background: linear-gradient(135deg, #2d2d2d 0%, #1a1a1a 100%); 
                border: 2px solid #ccaa77; 
                border-radius: 12px; 
                padding: 2rem; 
                text-align: center;
                margin-bottom: 1rem;">
        <div style="font-size: 3rem; margin-bottom: 1rem;">üìñ</div>
        <h4 style="color: #ccaa77; font-family: 'Cinzel', serif; margin-bottom: 0.5rem;">A Projektr≈ël</h4>
        <p style="color: #e8dcc8; font-size: 0.95rem; opacity: 0.8;">T√∂rt√©net, m√≥dszertan √©s forr√°sok</p>
    </div>
    """, unsafe_allow_html=True)
    if st.button("üìñ Tov√°bb a Projektr≈ël oldalra", key="nav_about", use_container_width=True):
        try:
            st.experimental_set_query_params(page="About")
            st.experimental_rerun()
        except Exception:
            pass

with nav_col2:
    st.markdown("""
    <div style="background: linear-gradient(135deg, #2d2d2d 0%, #1a1a1a 100%); 
                border: 2px solid #ccaa77; 
                border-radius: 12px; 
                padding: 2rem; 
                text-align: center;
                margin-bottom: 1rem;">
        <div style="font-size: 3rem; margin-bottom: 1rem;">üìä</div>
        <h4 style="color: #ccaa77; font-family: 'Cinzel', serif; margin-bottom: 0.5rem;">Analitika Dashboard</h4>
        <p style="color: #e8dcc8; font-size: 0.95rem; opacity: 0.8;">R√©szletes statisztik√°k √©s eloszl√°sok</p>
    </div>
    """, unsafe_allow_html=True)
    if st.button("üìñ Tov√°bb az elemz≈ëi oldalra", key="nav_analytics", use_container_width=True):
        try:
            st.experimental_set_query_params(page="analytics")
            st.experimental_rerun()
        except Exception:
            pass

st.markdown("""
<p style="text-align: center; color: #888; font-size: 0.9rem; margin-top: 1.5rem;">
    üí° <em>Vagy haszn√°ld a bal fels≈ë sarokban l√©v≈ë men√ºt (>>) a navig√°l√°shoz!</em>
</p>
""", unsafe_allow_html=True)

st.markdown(textwrap.dedent("""
<div style="text-align: center; padding: 3.5rem 2.5rem; background: linear-gradient(145deg, #1a0d0d 0%, #2b0f12 100%); color: #f5efe6; margin-top: 5rem; border-radius: 20px; border: 2px solid #ccaa77; box-shadow: 0 12px 40px rgba(0,0,0,0.6);">
    <p style="font-family: 'Cinzel', serif; font-size: 1.6rem; letter-spacing: 0.08em; margin-bottom: 0.3rem; color: #e8c896; text-shadow: 0 2px 6px rgba(0,0,0,0.8);">K√∂zrendek √çzh√°l√≥ja</p>
    <div style="width: 120px; height: 2px; background: linear-gradient(90deg, transparent, #ccaa77, transparent); margin: 0.8rem auto 1.2rem auto;"></div>
    <p style="font-family: 'Crimson Text', serif; font-size: 1.05rem; opacity: 0.9; margin: 0.2rem 0 1.6rem 0; letter-spacing: 0.04em;">H√°l√≥zatelemz√©s ‚Ä¢ T√∂rt√©neti forr√°sok ‚Ä¢ AI-alap√∫ gener√°l√°s</p>
    <p style="font-size: 0.95rem; line-height: 1.7; max-width: 820px; margin: 0 auto; opacity: 0.85; color: #efe6d8;">
        A projekt Barab√°si Albert-L√°szl√≥ h√°l√≥zatkutat√°saira √©s a
        <em>‚ÄûSzak√°csmesters√©gnek k√∂nyvecsk√©je"</em> (T√≥tfalusi Kis Mikl√≥s, 1698)
        c√≠m≈± szak√°csk√∂nyv digit√°lis elemz√©s√©re √©p√ºl.<br>
        Forr√°s: Magyar Elektronikus K√∂nyvt√°r (MEK), Orsz√°gos Sz√©ch√©nyi K√∂nyvt√°r
    </p>
    <p style="font-size: 0.9rem; margin-top: 1.4rem; opacity: 0.75; color: #d6b98c; letter-spacing: 0.06em;">
        Felhaszn√°lt technol√≥gi√°k: Streamlit ‚Ä¢ NetworkX ‚Ä¢ Plotly ‚Ä¢ SciPy ‚Ä¢ OpenAI GPT-5.1; 5-nano; 5-mini ‚Ä¢ Claude ‚Ä¢ Grok
    </p>
    <div style="width: 100%; height: 1px; background: linear-gradient(90deg, transparent, rgba(204,170,119,0.4), transparent); margin: 2rem 0 1.2rem 0;"></div>
    <p style="font-size: 0.85rem; opacity: 0.55; letter-spacing: 0.05em; color: #cbb58a;">
        ¬© 2025 ‚Ä¢ Digit√°lis b√∂lcs√©szet-, t√°rsadalom- √©s h√°l√≥zattudom√°nyi projekt
    </p>
</div>
"""), unsafe_allow_html=True)


